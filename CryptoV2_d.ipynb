{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajakumar05032000/Stock-price-prediction/blob/main/CryptoV2_d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBNEAhTKbyva"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib as pyplt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JQrMFs8b8Eg"
      },
      "outputs": [],
      "source": [
        "from tvDatafeed import TvDatafeed,Interval\n",
        "tv=TvDatafeed(auto_login=False, chromedriver_path='/usr/bin/chromedriver')\n",
        "data = tv.get_hist('BTCUSDT','BINANCE',interval=Interval.in_5_minute,n_bars=100000)\n",
        "data.to_csv('btc_5min_new.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9b20PsdTcC0Y",
        "outputId": "4fc93fb2-9680-406e-b8b1-5b615cde33f2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>datetime</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-11-23 06:50:00</th>\n",
              "      <td>BINANCE:BTCUSDT</td>\n",
              "      <td>56500.00</td>\n",
              "      <td>56783.96</td>\n",
              "      <td>56459.76</td>\n",
              "      <td>56750.00</td>\n",
              "      <td>108.59731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-23 06:55:00</th>\n",
              "      <td>BINANCE:BTCUSDT</td>\n",
              "      <td>56749.99</td>\n",
              "      <td>56989.17</td>\n",
              "      <td>56672.11</td>\n",
              "      <td>56741.21</td>\n",
              "      <td>267.90859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-23 07:00:00</th>\n",
              "      <td>BINANCE:BTCUSDT</td>\n",
              "      <td>56741.21</td>\n",
              "      <td>56822.33</td>\n",
              "      <td>56557.93</td>\n",
              "      <td>56561.69</td>\n",
              "      <td>114.76589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-23 07:05:00</th>\n",
              "      <td>BINANCE:BTCUSDT</td>\n",
              "      <td>56561.68</td>\n",
              "      <td>56610.99</td>\n",
              "      <td>56500.00</td>\n",
              "      <td>56532.25</td>\n",
              "      <td>214.08150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-23 07:10:00</th>\n",
              "      <td>BINANCE:BTCUSDT</td>\n",
              "      <td>56532.25</td>\n",
              "      <td>56636.54</td>\n",
              "      <td>56437.65</td>\n",
              "      <td>56603.10</td>\n",
              "      <td>90.03206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              symbol      open      high       low     close  \\\n",
              "datetime                                                                       \n",
              "2021-11-23 06:50:00  BINANCE:BTCUSDT  56500.00  56783.96  56459.76  56750.00   \n",
              "2021-11-23 06:55:00  BINANCE:BTCUSDT  56749.99  56989.17  56672.11  56741.21   \n",
              "2021-11-23 07:00:00  BINANCE:BTCUSDT  56741.21  56822.33  56557.93  56561.69   \n",
              "2021-11-23 07:05:00  BINANCE:BTCUSDT  56561.68  56610.99  56500.00  56532.25   \n",
              "2021-11-23 07:10:00  BINANCE:BTCUSDT  56532.25  56636.54  56437.65  56603.10   \n",
              "\n",
              "                        volume  \n",
              "datetime                        \n",
              "2021-11-23 06:50:00  108.59731  \n",
              "2021-11-23 06:55:00  267.90859  \n",
              "2021-11-23 07:00:00  114.76589  \n",
              "2021-11-23 07:05:00  214.08150  \n",
              "2021-11-23 07:10:00   90.03206  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsNkf7flcC8Q"
      },
      "outputs": [],
      "source": [
        "df_high=data.reset_index()['high']\n",
        "df_open=data.reset_index()['open']\n",
        "df_close=data.reset_index()['close']\n",
        "df_volume=data.reset_index()['volume']\n",
        "df_low=data.reset_index()['low']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KuLp9akcDDE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler_high = MinMaxScaler(feature_range=(0,1))\n",
        "scaler_open = MinMaxScaler(feature_range=(0,1))\n",
        "scaler_volume = MinMaxScaler(feature_range=(0,1))\n",
        "scaler_low = MinMaxScaler(feature_range=(0,1))\n",
        "scaler_close = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "df_high = scaler_high.fit_transform(np.array(df_high).reshape(-1,1))\n",
        "df_open = scaler_open.fit_transform(np.array(df_open).reshape(-1,1))\n",
        "df_volume = scaler_volume.fit_transform(np.array(df_volume).reshape(-1,1))\n",
        "df_low = scaler_low.fit_transform(np.array(df_low).reshape(-1,1))\n",
        "df_close = scaler_close.fit_transform(np.array(df_close).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QmTZv7gcDGI"
      },
      "outputs": [],
      "source": [
        "training_size=int(len(df_high)*0.80)\n",
        "test_size=len(df_high)-training_size\n",
        "\n",
        "\n",
        "train_data_high,test_data_high = df_high[0:training_size,:],df_high[training_size:len(df_high),:1]\n",
        "train_data_open,test_data_open = df_open[0:training_size,:],df_open[training_size:len(df_open),:1]\n",
        "train_data_close,test_data_close = df_close[0:training_size,:],df_close[training_size:len(df_close),:1]\n",
        "train_data_volume,test_data_volume = df_volume[0:training_size,:],df_volume[training_size:len(df_volume),:1]\n",
        "train_data_low,test_data_low = df_low[0:training_size,:],df_low[training_size:len(df_low),:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvS7l7K2cDJb"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "def create_dataset(dataset, time_step=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-time_step-1):\n",
        "\t\ta = dataset[i:(i+time_step), 0]  \n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + time_step, 0])\n",
        "\treturn numpy.array(dataX), numpy.array(dataY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTSrOXlecDMo"
      },
      "outputs": [],
      "source": [
        "\n",
        "time_step = 100\n",
        "\n",
        "X_train_high, y_train_high = create_dataset(train_data_high, time_step)\n",
        "X_test_high, y_test_high = create_dataset(test_data_high, time_step)\n",
        "\n",
        "X_train_open, y_train_open = create_dataset(train_data_open, time_step)\n",
        "X_test_open, y_test_open = create_dataset(test_data_open, time_step)\n",
        "\n",
        "X_train_close, y_train_close = create_dataset(train_data_close, time_step)\n",
        "X_test_close, y_test_close = create_dataset(test_data_close, time_step)\n",
        "\n",
        "X_train_volume, y_train_volume = create_dataset(train_data_volume, time_step)\n",
        "X_test_volume, y_test_volume = create_dataset(test_data_volume, time_step)\n",
        "\n",
        "X_train_low, y_train_low = create_dataset(train_data_low, time_step)\n",
        "X_test_low, y_test_low = create_dataset(test_data_low, time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YuuXe2nDOlz"
      },
      "outputs": [],
      "source": [
        "X_test_close = X_test_close.reshape(X_test_close.shape[0],X_test_close.shape[1] , 1)\n",
        "\n",
        "###### newly added ##################################\n",
        "X_test_open = X_test_open.reshape(X_test_open.shape[0],X_test_open.shape[1] , 1)\n",
        "X_test_high = X_test_high.reshape(X_test_high.shape[0],X_test_high.shape[1] , 1)\n",
        "X_test_volume = X_test_volume.reshape(X_test_volume.shape[0],X_test_volume.shape[1] , 1)\n",
        "X_test_low = X_test_low.reshape(X_test_low.shape[0],X_test_low.shape[1] , 1)\n",
        "\n",
        "\n",
        "######################################################\n",
        "X_train_high = X_train_high.reshape(X_train_high.shape[0],X_train_high.shape[1] , 1)\n",
        "X_train_open = X_train_open.reshape(X_train_open.shape[0],X_train_open.shape[1] , 1)\n",
        "X_train_close = X_train_close.reshape(X_train_close.shape[0],X_train_close.shape[1] , 1)\n",
        "X_train_volume = X_train_volume.reshape(X_train_volume.shape[0],X_train_volume.shape[1] , 1)\n",
        "X_train_low = X_train_low.reshape(X_train_low.shape[0],X_train_low.shape[1] , 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYAZLZ2WFIMt",
        "outputId": "f1804a46-cb9e-4a0a-9a62-17976cfeb5c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 1)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_close[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNmcWgFJEoU3"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional,GlobalMaxPool1D,Conv1D,MaxPool1D\n",
        "from keras.layers import LSTM,Input,Dense,Dropout,Activation,GRU,Concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# inp_high = Input(shape=(X_train_close[0].shape))\n",
        "# a = LSTM(50,return_sequences=True)(inp_high)#50 \n",
        "# a = LSTM(25,return_sequences=True)(a)#25  \n",
        "# a = LSTM(20)(a)#20\n",
        "# a = Dense(1)(a)#1\n",
        "\n",
        "inp_open = Input(shape=(X_train_close[0].shape))\n",
        "b = Bidirectional(LSTM(50,return_sequences=True))(inp_open)\n",
        "b = Bidirectional(LSTM(25,return_sequences=True))(b)\n",
        "b = Bidirectional(LSTM(20))(b)\n",
        "b = Dense(1)(b)\n",
        "\n",
        "inp_close = Input(shape=(X_train_close[0].shape))\n",
        "c = Bidirectional(LSTM(150,return_sequences=True))(inp_close)\n",
        "c = Bidirectional(LSTM(50,return_sequences=True))(c)\n",
        "c = Bidirectional(LSTM(30))(c)\n",
        "c = Dense(1)(c)\n",
        "\n",
        "inp_volume = Input(shape=(X_train_close[0].shape))\n",
        "d = Bidirectional(LSTM(50,return_sequences=True))(inp_volume)\n",
        "d = Bidirectional(LSTM(25,return_sequences=True))(d)\n",
        "d = Bidirectional(LSTM(20))(d)\n",
        "d = Dense(1)(d)\n",
        "\n",
        "# inp_low = Input(shape=(X_train_close[0].shape))\n",
        "# e = LSTM(50,return_sequences=True)(inp_low)\n",
        "# e = LSTM(25,return_sequences=True)(e)\n",
        "# e = LSTM(20)(e)\n",
        "# e = Dense(1)(e)\n",
        "\n",
        "\n",
        "\n",
        "conc= Concatenate()([b,c,d])\n",
        "out=Dense(1)(conc)\n",
        "\n",
        "\n",
        "#model = Model([inp_open,inp_close,inp_volume], out)\n",
        "model = Model(inp_close,c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJJ4P7RWHTwn"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(filepath='best_5min.hdf5', \n",
        "                             monitor='val_loss',\n",
        "                             verbose=1, \n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "callbacks = [checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8QngltIHqri",
        "outputId": "c3dda30a-9c61-4e90-8025-d474bb25eaea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0215\n",
            "Epoch 00001: val_loss improved from inf to 0.00082, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 46s 604ms/step - loss: 0.0215 - val_loss: 8.1560e-04\n",
            "Epoch 2/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.0018\n",
            "Epoch 00002: val_loss improved from 0.00082 to 0.00034, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 36s 583ms/step - loss: 0.0018 - val_loss: 3.4110e-04\n",
            "Epoch 3/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.2919e-04\n",
            "Epoch 00003: val_loss improved from 0.00034 to 0.00028, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 35s 578ms/step - loss: 6.2919e-04 - val_loss: 2.7772e-04\n",
            "Epoch 4/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.2066e-04\n",
            "Epoch 00004: val_loss improved from 0.00028 to 0.00026, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 35s 574ms/step - loss: 5.2066e-04 - val_loss: 2.6238e-04\n",
            "Epoch 5/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0214e-04\n",
            "Epoch 00005: val_loss did not improve from 0.00026\n",
            "61/61 [==============================] - 35s 580ms/step - loss: 5.0214e-04 - val_loss: 2.6458e-04\n",
            "Epoch 6/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8959e-04\n",
            "Epoch 00006: val_loss improved from 0.00026 to 0.00024, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 39s 640ms/step - loss: 4.8959e-04 - val_loss: 2.3800e-04\n",
            "Epoch 7/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4894e-04\n",
            "Epoch 00007: val_loss did not improve from 0.00024\n",
            "61/61 [==============================] - 35s 572ms/step - loss: 4.4894e-04 - val_loss: 2.4138e-04\n",
            "Epoch 8/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5819e-04\n",
            "Epoch 00008: val_loss improved from 0.00024 to 0.00022, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 35s 577ms/step - loss: 4.5819e-04 - val_loss: 2.1893e-04\n",
            "Epoch 9/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9345e-04\n",
            "Epoch 00009: val_loss improved from 0.00022 to 0.00021, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 35s 580ms/step - loss: 3.9345e-04 - val_loss: 2.0806e-04\n",
            "Epoch 10/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.1670e-04\n",
            "Epoch 00010: val_loss improved from 0.00021 to 0.00020, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 562ms/step - loss: 4.1670e-04 - val_loss: 2.0331e-04\n",
            "Epoch 11/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7001e-04\n",
            "Epoch 00011: val_loss improved from 0.00020 to 0.00019, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 563ms/step - loss: 3.7001e-04 - val_loss: 1.9126e-04\n",
            "Epoch 12/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.5877e-04\n",
            "Epoch 00012: val_loss improved from 0.00019 to 0.00019, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 38s 624ms/step - loss: 3.5877e-04 - val_loss: 1.8889e-04\n",
            "Epoch 13/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7916e-04\n",
            "Epoch 00013: val_loss did not improve from 0.00019\n",
            "61/61 [==============================] - 35s 579ms/step - loss: 3.7916e-04 - val_loss: 2.1887e-04\n",
            "Epoch 14/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.4982e-04\n",
            "Epoch 00014: val_loss improved from 0.00019 to 0.00017, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 565ms/step - loss: 3.4982e-04 - val_loss: 1.7411e-04\n",
            "Epoch 15/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.3779e-04\n",
            "Epoch 00015: val_loss did not improve from 0.00017\n",
            "61/61 [==============================] - 36s 587ms/step - loss: 3.3779e-04 - val_loss: 1.7468e-04\n",
            "Epoch 16/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7953e-04\n",
            "Epoch 00016: val_loss did not improve from 0.00017\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 3.7953e-04 - val_loss: 1.7619e-04\n",
            "Epoch 17/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7329e-04\n",
            "Epoch 00017: val_loss improved from 0.00017 to 0.00016, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 35s 580ms/step - loss: 3.7329e-04 - val_loss: 1.6421e-04\n",
            "Epoch 18/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.9502e-04\n",
            "Epoch 00018: val_loss did not improve from 0.00016\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 2.9502e-04 - val_loss: 1.6823e-04\n",
            "Epoch 19/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0017e-04\n",
            "Epoch 00019: val_loss did not improve from 0.00016\n",
            "61/61 [==============================] - 35s 579ms/step - loss: 3.0017e-04 - val_loss: 1.7411e-04\n",
            "Epoch 20/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.9902e-04\n",
            "Epoch 00020: val_loss improved from 0.00016 to 0.00016, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 2.9902e-04 - val_loss: 1.5893e-04\n",
            "Epoch 21/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8597e-04\n",
            "Epoch 00021: val_loss did not improve from 0.00016\n",
            "61/61 [==============================] - 34s 561ms/step - loss: 2.8597e-04 - val_loss: 1.7904e-04\n",
            "Epoch 22/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7189e-04\n",
            "Epoch 00022: val_loss did not improve from 0.00016\n",
            "61/61 [==============================] - 34s 559ms/step - loss: 2.7189e-04 - val_loss: 1.6182e-04\n",
            "Epoch 23/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7820e-04\n",
            "Epoch 00023: val_loss improved from 0.00016 to 0.00014, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 562ms/step - loss: 2.7820e-04 - val_loss: 1.4383e-04\n",
            "Epoch 24/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5391e-04\n",
            "Epoch 00024: val_loss improved from 0.00014 to 0.00014, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 2.5391e-04 - val_loss: 1.4197e-04\n",
            "Epoch 25/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.4499e-04\n",
            "Epoch 00025: val_loss did not improve from 0.00014\n",
            "61/61 [==============================] - 34s 561ms/step - loss: 3.4499e-04 - val_loss: 1.5781e-04\n",
            "Epoch 26/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3999e-04\n",
            "Epoch 00026: val_loss improved from 0.00014 to 0.00014, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 562ms/step - loss: 2.3999e-04 - val_loss: 1.3641e-04\n",
            "Epoch 27/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3857e-04\n",
            "Epoch 00027: val_loss improved from 0.00014 to 0.00014, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 2.3857e-04 - val_loss: 1.3533e-04\n",
            "Epoch 28/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5159e-04\n",
            "Epoch 00028: val_loss improved from 0.00014 to 0.00013, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 563ms/step - loss: 2.5159e-04 - val_loss: 1.3163e-04\n",
            "Epoch 29/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6494e-04\n",
            "Epoch 00029: val_loss did not improve from 0.00013\n",
            "61/61 [==============================] - 34s 559ms/step - loss: 2.6494e-04 - val_loss: 1.4768e-04\n",
            "Epoch 30/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3392e-04\n",
            "Epoch 00030: val_loss improved from 0.00013 to 0.00012, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 560ms/step - loss: 2.3392e-04 - val_loss: 1.2340e-04\n",
            "Epoch 31/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5194e-04\n",
            "Epoch 00031: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 560ms/step - loss: 2.5194e-04 - val_loss: 1.3612e-04\n",
            "Epoch 32/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1777e-04\n",
            "Epoch 00032: val_loss improved from 0.00012 to 0.00012, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 561ms/step - loss: 2.1777e-04 - val_loss: 1.2304e-04\n",
            "Epoch 33/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1532e-04\n",
            "Epoch 00033: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 559ms/step - loss: 2.1532e-04 - val_loss: 1.6674e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0766e-04\n",
            "Epoch 00034: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 563ms/step - loss: 2.0766e-04 - val_loss: 1.2556e-04\n",
            "Epoch 35/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1448e-04\n",
            "Epoch 00035: val_loss improved from 0.00012 to 0.00012, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 565ms/step - loss: 2.1448e-04 - val_loss: 1.1711e-04\n",
            "Epoch 36/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6707e-04\n",
            "Epoch 00036: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 563ms/step - loss: 2.6707e-04 - val_loss: 1.8053e-04\n",
            "Epoch 37/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.3298e-04\n",
            "Epoch 00037: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 562ms/step - loss: 3.3298e-04 - val_loss: 1.2243e-04\n",
            "Epoch 38/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2735e-04\n",
            "Epoch 00038: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 558ms/step - loss: 2.2735e-04 - val_loss: 1.1967e-04\n",
            "Epoch 39/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9747e-04\n",
            "Epoch 00039: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 558ms/step - loss: 1.9747e-04 - val_loss: 1.4996e-04\n",
            "Epoch 40/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8770e-04\n",
            "Epoch 00040: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 558ms/step - loss: 1.8770e-04 - val_loss: 1.4189e-04\n",
            "Epoch 41/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0711e-04\n",
            "Epoch 00041: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 561ms/step - loss: 2.0711e-04 - val_loss: 1.1879e-04\n",
            "Epoch 42/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8397e-04\n",
            "Epoch 00042: val_loss did not improve from 0.00012\n",
            "61/61 [==============================] - 34s 566ms/step - loss: 1.8397e-04 - val_loss: 1.1780e-04\n",
            "Epoch 43/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0312e-04\n",
            "Epoch 00043: val_loss improved from 0.00012 to 0.00011, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 2.0312e-04 - val_loss: 1.0819e-04\n",
            "Epoch 44/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8716e-04\n",
            "Epoch 00044: val_loss did not improve from 0.00011\n",
            "61/61 [==============================] - 35s 566ms/step - loss: 1.8716e-04 - val_loss: 1.5215e-04\n",
            "Epoch 45/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1005e-04\n",
            "Epoch 00045: val_loss did not improve from 0.00011\n",
            "61/61 [==============================] - 34s 560ms/step - loss: 2.1005e-04 - val_loss: 1.2314e-04\n",
            "Epoch 46/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8633e-04\n",
            "Epoch 00046: val_loss improved from 0.00011 to 0.00010, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 563ms/step - loss: 1.8633e-04 - val_loss: 1.0335e-04\n",
            "Epoch 47/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0332e-04\n",
            "Epoch 00047: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 34s 559ms/step - loss: 2.0332e-04 - val_loss: 1.1833e-04\n",
            "Epoch 48/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8026e-04\n",
            "Epoch 00048: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 37s 602ms/step - loss: 1.8026e-04 - val_loss: 1.3182e-04\n",
            "Epoch 49/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6745e-04\n",
            "Epoch 00049: val_loss improved from 0.00010 to 0.00010, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 35s 577ms/step - loss: 1.6745e-04 - val_loss: 1.0090e-04\n",
            "Epoch 50/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6507e-04\n",
            "Epoch 00050: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 35s 575ms/step - loss: 1.6507e-04 - val_loss: 1.0558e-04\n",
            "Epoch 51/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7810e-04\n",
            "Epoch 00051: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 37s 610ms/step - loss: 1.7810e-04 - val_loss: 1.2823e-04\n",
            "Epoch 52/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0891e-04\n",
            "Epoch 00052: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 34s 558ms/step - loss: 2.0891e-04 - val_loss: 1.5475e-04\n",
            "Epoch 53/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2207e-04\n",
            "Epoch 00053: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 2.2207e-04 - val_loss: 1.8076e-04\n",
            "Epoch 54/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0573e-04\n",
            "Epoch 00054: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 35s 572ms/step - loss: 2.0573e-04 - val_loss: 1.1285e-04\n",
            "Epoch 55/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5424e-04\n",
            "Epoch 00055: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 35s 576ms/step - loss: 1.5424e-04 - val_loss: 1.1489e-04\n",
            "Epoch 56/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6110e-04\n",
            "Epoch 00056: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.6110e-04 - val_loss: 1.4943e-04\n",
            "Epoch 57/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7677e-04\n",
            "Epoch 00057: val_loss did not improve from 0.00010\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.7677e-04 - val_loss: 1.0517e-04\n",
            "Epoch 58/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9935e-04\n",
            "Epoch 00058: val_loss improved from 0.00010 to 0.00009, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.9935e-04 - val_loss: 9.3458e-05\n",
            "Epoch 59/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8233e-04\n",
            "Epoch 00059: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 569ms/step - loss: 1.8233e-04 - val_loss: 9.9659e-05\n",
            "Epoch 60/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5435e-04\n",
            "Epoch 00060: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 566ms/step - loss: 1.5435e-04 - val_loss: 1.0696e-04\n",
            "Epoch 61/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3525e-04\n",
            "Epoch 00061: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 2.3525e-04 - val_loss: 1.6218e-04\n",
            "Epoch 62/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5468e-04\n",
            "Epoch 00062: val_loss improved from 0.00009 to 0.00009, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 35s 571ms/step - loss: 1.5468e-04 - val_loss: 9.3223e-05\n",
            "Epoch 63/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7290e-04\n",
            "Epoch 00063: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.7290e-04 - val_loss: 9.7225e-05\n",
            "Epoch 64/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7637e-04\n",
            "Epoch 00064: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 579ms/step - loss: 1.7637e-04 - val_loss: 9.6196e-05\n",
            "Epoch 65/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6478e-04\n",
            "Epoch 00065: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 572ms/step - loss: 1.6478e-04 - val_loss: 1.1138e-04\n",
            "Epoch 66/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6356e-04\n",
            "Epoch 00066: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.6356e-04 - val_loss: 9.8855e-05\n",
            "Epoch 67/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5981e-04\n",
            "Epoch 00067: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 569ms/step - loss: 1.5981e-04 - val_loss: 9.5707e-05\n",
            "Epoch 68/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5401e-04\n",
            "Epoch 00068: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 569ms/step - loss: 1.5401e-04 - val_loss: 1.3586e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5088e-04\n",
            "Epoch 00069: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 35s 578ms/step - loss: 1.5088e-04 - val_loss: 1.0786e-04\n",
            "Epoch 70/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7500e-04\n",
            "Epoch 00070: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 36s 596ms/step - loss: 1.7500e-04 - val_loss: 1.1585e-04\n",
            "Epoch 71/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5801e-04\n",
            "Epoch 00071: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 36s 583ms/step - loss: 1.5801e-04 - val_loss: 1.1095e-04\n",
            "Epoch 72/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6181e-04\n",
            "Epoch 00072: val_loss improved from 0.00009 to 0.00009, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 43s 708ms/step - loss: 1.6181e-04 - val_loss: 8.8745e-05\n",
            "Epoch 73/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5522e-04\n",
            "Epoch 00073: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 41s 671ms/step - loss: 1.5522e-04 - val_loss: 1.1542e-04\n",
            "Epoch 74/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5493e-04\n",
            "Epoch 00074: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 42s 692ms/step - loss: 1.5493e-04 - val_loss: 9.3510e-05\n",
            "Epoch 75/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0116e-04\n",
            "Epoch 00075: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 43s 706ms/step - loss: 2.0116e-04 - val_loss: 1.2859e-04\n",
            "Epoch 76/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5796e-04\n",
            "Epoch 00076: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 43s 701ms/step - loss: 1.5796e-04 - val_loss: 1.2404e-04\n",
            "Epoch 77/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7145e-04\n",
            "Epoch 00077: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 42s 691ms/step - loss: 1.7145e-04 - val_loss: 9.9374e-05\n",
            "Epoch 78/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6133e-04\n",
            "Epoch 00078: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 42s 686ms/step - loss: 1.6133e-04 - val_loss: 1.1274e-04\n",
            "Epoch 79/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5938e-04\n",
            "Epoch 00079: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 42s 682ms/step - loss: 1.5938e-04 - val_loss: 1.3951e-04\n",
            "Epoch 80/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3256e-04\n",
            "Epoch 00080: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 42s 685ms/step - loss: 1.3256e-04 - val_loss: 9.7862e-05\n",
            "Epoch 81/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5424e-04\n",
            "Epoch 00081: val_loss did not improve from 0.00009\n",
            "61/61 [==============================] - 41s 675ms/step - loss: 1.5424e-04 - val_loss: 8.9963e-05\n",
            "Epoch 82/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4216e-04\n",
            "Epoch 00082: val_loss improved from 0.00009 to 0.00008, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 41s 667ms/step - loss: 1.4216e-04 - val_loss: 8.2557e-05\n",
            "Epoch 83/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3171e-04\n",
            "Epoch 00083: val_loss did not improve from 0.00008\n",
            "61/61 [==============================] - 40s 652ms/step - loss: 1.3171e-04 - val_loss: 1.0112e-04\n",
            "Epoch 84/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5606e-04\n",
            "Epoch 00084: val_loss did not improve from 0.00008\n",
            "61/61 [==============================] - 40s 655ms/step - loss: 1.5606e-04 - val_loss: 1.5045e-04\n",
            "Epoch 85/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3774e-04\n",
            "Epoch 00085: val_loss did not improve from 0.00008\n",
            "61/61 [==============================] - 40s 658ms/step - loss: 1.3774e-04 - val_loss: 9.0995e-05\n",
            "Epoch 86/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3608e-04\n",
            "Epoch 00086: val_loss improved from 0.00008 to 0.00008, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 40s 655ms/step - loss: 1.3608e-04 - val_loss: 7.9979e-05\n",
            "Epoch 87/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6019e-04\n",
            "Epoch 00087: val_loss did not improve from 0.00008\n",
            "61/61 [==============================] - 40s 650ms/step - loss: 1.6019e-04 - val_loss: 1.9241e-04\n",
            "Epoch 88/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4245e-04\n",
            "Epoch 00088: val_loss did not improve from 0.00008\n",
            "61/61 [==============================] - 40s 651ms/step - loss: 1.4245e-04 - val_loss: 1.1139e-04\n",
            "Epoch 89/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5604e-04\n",
            "Epoch 00089: val_loss did not improve from 0.00008\n",
            "61/61 [==============================] - 40s 652ms/step - loss: 1.5604e-04 - val_loss: 8.7585e-05\n",
            "Epoch 90/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3825e-04\n",
            "Epoch 00090: val_loss did not improve from 0.00008\n",
            "61/61 [==============================] - 40s 650ms/step - loss: 1.3825e-04 - val_loss: 8.1665e-05\n",
            "Epoch 91/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4863e-04\n",
            "Epoch 00091: val_loss did not improve from 0.00008\n",
            "61/61 [==============================] - 35s 577ms/step - loss: 1.4863e-04 - val_loss: 8.9838e-05\n",
            "Epoch 92/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3490e-04\n",
            "Epoch 00092: val_loss improved from 0.00008 to 0.00008, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 34s 562ms/step - loss: 1.3490e-04 - val_loss: 7.9797e-05\n",
            "Epoch 93/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4018e-04\n",
            "Epoch 00093: val_loss improved from 0.00008 to 0.00007, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 41s 674ms/step - loss: 1.4018e-04 - val_loss: 7.4836e-05\n",
            "Epoch 94/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4517e-04\n",
            "Epoch 00094: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 40s 653ms/step - loss: 1.4517e-04 - val_loss: 7.8592e-05\n",
            "Epoch 95/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4740e-04\n",
            "Epoch 00095: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 40s 659ms/step - loss: 1.4740e-04 - val_loss: 9.6234e-05\n",
            "Epoch 96/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4961e-04\n",
            "Epoch 00096: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 42s 683ms/step - loss: 1.4961e-04 - val_loss: 1.0024e-04\n",
            "Epoch 97/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4308e-04\n",
            "Epoch 00097: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 38s 630ms/step - loss: 1.4308e-04 - val_loss: 8.0521e-05\n",
            "Epoch 98/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3056e-04\n",
            "Epoch 00098: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 43s 704ms/step - loss: 1.3056e-04 - val_loss: 9.3392e-05\n",
            "Epoch 99/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7098e-04\n",
            "Epoch 00099: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 40s 656ms/step - loss: 1.7098e-04 - val_loss: 1.0101e-04\n",
            "Epoch 100/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4821e-04\n",
            "Epoch 00100: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 42s 689ms/step - loss: 1.4821e-04 - val_loss: 7.5659e-05\n",
            "Epoch 101/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6230e-04\n",
            "Epoch 00101: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 41s 671ms/step - loss: 1.6230e-04 - val_loss: 2.6506e-04\n",
            "Epoch 102/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5615e-04\n",
            "Epoch 00102: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 41s 675ms/step - loss: 1.5615e-04 - val_loss: 8.3515e-05\n",
            "Epoch 103/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4882e-04\n",
            "Epoch 00103: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 41s 668ms/step - loss: 1.4882e-04 - val_loss: 7.6507e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 104/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2266e-04\n",
            "Epoch 00104: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 40s 661ms/step - loss: 1.2266e-04 - val_loss: 8.4065e-05\n",
            "Epoch 105/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3072e-04\n",
            "Epoch 00105: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 41s 666ms/step - loss: 1.3072e-04 - val_loss: 8.5590e-05\n",
            "Epoch 106/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2765e-04\n",
            "Epoch 00106: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 43s 702ms/step - loss: 1.2765e-04 - val_loss: 7.5564e-05\n",
            "Epoch 107/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2154e-04\n",
            "Epoch 00107: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 37s 613ms/step - loss: 1.2154e-04 - val_loss: 1.1456e-04\n",
            "Epoch 108/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1820e-04\n",
            "Epoch 00108: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 729ms/step - loss: 1.1820e-04 - val_loss: 8.0229e-05\n",
            "Epoch 109/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1959e-04\n",
            "Epoch 00109: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 46s 751ms/step - loss: 1.1959e-04 - val_loss: 1.3019e-04\n",
            "Epoch 110/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2221e-04\n",
            "Epoch 00110: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 46s 755ms/step - loss: 1.2221e-04 - val_loss: 7.5094e-05\n",
            "Epoch 111/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2510e-04\n",
            "Epoch 00111: val_loss improved from 0.00007 to 0.00007, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 46s 759ms/step - loss: 1.2510e-04 - val_loss: 7.3085e-05\n",
            "Epoch 112/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5436e-04\n",
            "Epoch 00112: val_loss improved from 0.00007 to 0.00007, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 48s 788ms/step - loss: 1.5436e-04 - val_loss: 7.0807e-05\n",
            "Epoch 113/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4914e-04\n",
            "Epoch 00113: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 46s 751ms/step - loss: 1.4914e-04 - val_loss: 9.3170e-05\n",
            "Epoch 114/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1551e-04\n",
            "Epoch 00114: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 46s 753ms/step - loss: 1.1551e-04 - val_loss: 1.0943e-04\n",
            "Epoch 115/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1355e-04\n",
            "Epoch 00115: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 36s 584ms/step - loss: 1.1355e-04 - val_loss: 8.8115e-05\n",
            "Epoch 116/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1014e-04\n",
            "Epoch 00116: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.1014e-04 - val_loss: 7.7083e-05\n",
            "Epoch 117/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1650e-04\n",
            "Epoch 00117: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 724ms/step - loss: 1.1650e-04 - val_loss: 7.6015e-05\n",
            "Epoch 118/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2408e-04\n",
            "Epoch 00118: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 46s 754ms/step - loss: 1.2408e-04 - val_loss: 7.6663e-05\n",
            "Epoch 119/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3785e-04\n",
            "Epoch 00119: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 46s 761ms/step - loss: 1.3785e-04 - val_loss: 7.2249e-05\n",
            "Epoch 120/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4254e-04\n",
            "Epoch 00120: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 47s 764ms/step - loss: 1.4254e-04 - val_loss: 8.9428e-05\n",
            "Epoch 121/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0878e-04\n",
            "Epoch 00121: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 45s 743ms/step - loss: 1.0878e-04 - val_loss: 1.1490e-04\n",
            "Epoch 122/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1652e-04\n",
            "Epoch 00122: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 43s 710ms/step - loss: 1.1652e-04 - val_loss: 2.0904e-04\n",
            "Epoch 123/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3043e-04\n",
            "Epoch 00123: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 45s 743ms/step - loss: 1.3043e-04 - val_loss: 9.0970e-05\n",
            "Epoch 124/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2111e-04\n",
            "Epoch 00124: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 47s 764ms/step - loss: 1.2111e-04 - val_loss: 7.4340e-05\n",
            "Epoch 125/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1119e-04\n",
            "Epoch 00125: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 51s 842ms/step - loss: 1.1119e-04 - val_loss: 8.3824e-05\n",
            "Epoch 126/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1334e-04\n",
            "Epoch 00126: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 48s 792ms/step - loss: 1.1334e-04 - val_loss: 9.8116e-05\n",
            "Epoch 127/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0692e-04\n",
            "Epoch 00127: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 47s 774ms/step - loss: 1.0692e-04 - val_loss: 7.1602e-05\n",
            "Epoch 128/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0478e-04\n",
            "Epoch 00128: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 45s 732ms/step - loss: 1.0478e-04 - val_loss: 9.0774e-05\n",
            "Epoch 129/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1109e-04\n",
            "Epoch 00129: val_loss improved from 0.00007 to 0.00007, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 44s 727ms/step - loss: 1.1109e-04 - val_loss: 6.9954e-05\n",
            "Epoch 130/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3394e-04\n",
            "Epoch 00130: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 45s 740ms/step - loss: 1.3394e-04 - val_loss: 8.7084e-05\n",
            "Epoch 131/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3105e-04\n",
            "Epoch 00131: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 45s 738ms/step - loss: 1.3105e-04 - val_loss: 8.9562e-05\n",
            "Epoch 132/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2546e-04\n",
            "Epoch 00132: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 50s 823ms/step - loss: 1.2546e-04 - val_loss: 7.3199e-05\n",
            "Epoch 133/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0790e-04\n",
            "Epoch 00133: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 49s 800ms/step - loss: 1.0790e-04 - val_loss: 9.6829e-05\n",
            "Epoch 134/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1210e-04\n",
            "Epoch 00134: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 53s 862ms/step - loss: 1.1210e-04 - val_loss: 9.4950e-05\n",
            "Epoch 135/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1086e-04\n",
            "Epoch 00135: val_loss improved from 0.00007 to 0.00007, saving model to best_5min.hdf5\n",
            "61/61 [==============================] - 51s 838ms/step - loss: 1.1086e-04 - val_loss: 6.5764e-05\n",
            "Epoch 136/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0968e-04\n",
            "Epoch 00136: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 40s 651ms/step - loss: 1.0968e-04 - val_loss: 7.2828e-05\n",
            "Epoch 137/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2066e-04\n",
            "Epoch 00137: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 1.2066e-04 - val_loss: 7.7979e-05\n",
            "Epoch 138/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2047e-04\n",
            "Epoch 00138: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 1.2047e-04 - val_loss: 8.4108e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 139/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0669e-04\n",
            "Epoch 00139: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 36s 593ms/step - loss: 1.0669e-04 - val_loss: 1.4581e-04\n",
            "Epoch 140/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2696e-04\n",
            "Epoch 00140: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 37s 604ms/step - loss: 1.2696e-04 - val_loss: 7.0518e-05\n",
            "Epoch 141/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1626e-04\n",
            "Epoch 00141: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 41s 668ms/step - loss: 1.1626e-04 - val_loss: 1.0903e-04\n",
            "Epoch 142/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0849e-04\n",
            "Epoch 00142: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.0849e-04 - val_loss: 7.9594e-05\n",
            "Epoch 143/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2956e-04\n",
            "Epoch 00143: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 36s 586ms/step - loss: 1.2956e-04 - val_loss: 1.2914e-04\n",
            "Epoch 144/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2092e-04\n",
            "Epoch 00144: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 43s 699ms/step - loss: 1.2092e-04 - val_loss: 1.2284e-04\n",
            "Epoch 145/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2125e-04\n",
            "Epoch 00145: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 721ms/step - loss: 1.2125e-04 - val_loss: 7.5952e-05\n",
            "Epoch 146/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0976e-04\n",
            "Epoch 00146: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 39s 634ms/step - loss: 1.0976e-04 - val_loss: 1.0697e-04\n",
            "Epoch 147/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1252e-04\n",
            "Epoch 00147: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 42s 690ms/step - loss: 1.1252e-04 - val_loss: 6.6759e-05\n",
            "Epoch 148/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3361e-04\n",
            "Epoch 00148: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 42s 691ms/step - loss: 1.3361e-04 - val_loss: 6.7039e-05\n",
            "Epoch 149/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0764e-04\n",
            "Epoch 00149: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 45s 742ms/step - loss: 1.0764e-04 - val_loss: 1.5557e-04\n",
            "Epoch 150/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1156e-04\n",
            "Epoch 00150: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 47s 777ms/step - loss: 1.1156e-04 - val_loss: 1.0224e-04\n",
            "Epoch 151/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1333e-04\n",
            "Epoch 00151: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 725ms/step - loss: 1.1333e-04 - val_loss: 7.5868e-05\n",
            "Epoch 152/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2713e-04\n",
            "Epoch 00152: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 52s 853ms/step - loss: 1.2713e-04 - val_loss: 1.9374e-04\n",
            "Epoch 153/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1393e-04\n",
            "Epoch 00153: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 42s 683ms/step - loss: 1.1393e-04 - val_loss: 1.5938e-04\n",
            "Epoch 154/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1151e-04\n",
            "Epoch 00154: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 40s 660ms/step - loss: 1.1151e-04 - val_loss: 1.1056e-04\n",
            "Epoch 155/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4662e-04\n",
            "Epoch 00155: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 38s 631ms/step - loss: 1.4662e-04 - val_loss: 8.2617e-05\n",
            "Epoch 156/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1238e-04\n",
            "Epoch 00156: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 48s 784ms/step - loss: 1.1238e-04 - val_loss: 9.7927e-05\n",
            "Epoch 157/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0698e-04\n",
            "Epoch 00157: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 727ms/step - loss: 1.0698e-04 - val_loss: 1.0627e-04\n",
            "Epoch 158/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0705e-04\n",
            "Epoch 00158: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 728ms/step - loss: 1.0705e-04 - val_loss: 9.0978e-05\n",
            "Epoch 159/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0546e-04\n",
            "Epoch 00159: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 41s 666ms/step - loss: 1.0546e-04 - val_loss: 7.5577e-05\n",
            "Epoch 160/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1461e-04\n",
            "Epoch 00160: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 40s 651ms/step - loss: 1.1461e-04 - val_loss: 8.9053e-05\n",
            "Epoch 161/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2020e-04\n",
            "Epoch 00161: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 43s 705ms/step - loss: 1.2020e-04 - val_loss: 9.0805e-05\n",
            "Epoch 162/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3531e-04\n",
            "Epoch 00162: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 720ms/step - loss: 1.3531e-04 - val_loss: 9.2216e-05\n",
            "Epoch 163/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5171e-04\n",
            "Epoch 00163: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 719ms/step - loss: 1.5171e-04 - val_loss: 1.0089e-04\n",
            "Epoch 164/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0651e-04\n",
            "Epoch 00164: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 43s 708ms/step - loss: 1.0651e-04 - val_loss: 1.0158e-04\n",
            "Epoch 165/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0173e-04\n",
            "Epoch 00165: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 43s 706ms/step - loss: 1.0173e-04 - val_loss: 7.9513e-05\n",
            "Epoch 166/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0571e-04\n",
            "Epoch 00166: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 42s 687ms/step - loss: 1.0571e-04 - val_loss: 6.7085e-05\n",
            "Epoch 167/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0222e-04\n",
            "Epoch 00167: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 46s 766ms/step - loss: 1.0222e-04 - val_loss: 8.2867e-05\n",
            "Epoch 168/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1175e-04\n",
            "Epoch 00168: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 721ms/step - loss: 1.1175e-04 - val_loss: 1.2228e-04\n",
            "Epoch 169/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1128e-04\n",
            "Epoch 00169: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 722ms/step - loss: 1.1128e-04 - val_loss: 7.6219e-05\n",
            "Epoch 170/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0670e-04\n",
            "Epoch 00170: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 721ms/step - loss: 1.0670e-04 - val_loss: 9.5746e-05\n",
            "Epoch 171/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0962e-04\n",
            "Epoch 00171: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 43s 711ms/step - loss: 1.0962e-04 - val_loss: 8.8427e-05\n",
            "Epoch 172/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1936e-04\n",
            "Epoch 00172: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 716ms/step - loss: 1.1936e-04 - val_loss: 1.3863e-04\n",
            "Epoch 173/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2030e-04\n",
            "Epoch 00173: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 720ms/step - loss: 1.2030e-04 - val_loss: 1.6956e-04\n",
            "Epoch 174/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1378e-04\n",
            "Epoch 00174: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 45s 739ms/step - loss: 1.1378e-04 - val_loss: 1.1376e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 175/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0841e-04\n",
            "Epoch 00175: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 724ms/step - loss: 1.0841e-04 - val_loss: 8.2287e-05\n",
            "Epoch 176/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1960e-04\n",
            "Epoch 00176: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 714ms/step - loss: 1.1960e-04 - val_loss: 1.1908e-04\n",
            "Epoch 177/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1383e-04\n",
            "Epoch 00177: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 37s 610ms/step - loss: 1.1383e-04 - val_loss: 1.5109e-04\n",
            "Epoch 178/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1361e-04\n",
            "Epoch 00178: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 45s 731ms/step - loss: 1.1361e-04 - val_loss: 8.1674e-05\n",
            "Epoch 179/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1704e-04\n",
            "Epoch 00179: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 44s 717ms/step - loss: 1.1704e-04 - val_loss: 1.3106e-04\n",
            "Epoch 180/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1484e-04\n",
            "Epoch 00180: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 40s 666ms/step - loss: 1.1484e-04 - val_loss: 2.2331e-04\n",
            "Epoch 181/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2795e-04\n",
            "Epoch 00181: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 40s 660ms/step - loss: 1.2795e-04 - val_loss: 1.2631e-04\n",
            "Epoch 182/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0343e-04\n",
            "Epoch 00182: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 49s 800ms/step - loss: 1.0343e-04 - val_loss: 1.2186e-04\n",
            "Epoch 183/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1390e-04\n",
            "Epoch 00183: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 39s 640ms/step - loss: 1.1390e-04 - val_loss: 1.2132e-04\n",
            "Epoch 184/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0337e-04\n",
            "Epoch 00184: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 43s 709ms/step - loss: 1.0337e-04 - val_loss: 8.1261e-05\n",
            "Epoch 185/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0145e-04\n",
            "Epoch 00185: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 36s 590ms/step - loss: 1.0145e-04 - val_loss: 9.4257e-05\n",
            "Epoch 186/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1181e-04\n",
            "Epoch 00186: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 569ms/step - loss: 1.1181e-04 - val_loss: 7.0653e-05\n",
            "Epoch 187/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0669e-04\n",
            "Epoch 00187: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 565ms/step - loss: 1.0669e-04 - val_loss: 8.2459e-05\n",
            "Epoch 188/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1327e-04\n",
            "Epoch 00188: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 36s 592ms/step - loss: 1.1327e-04 - val_loss: 7.8739e-05\n",
            "Epoch 189/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0265e-04\n",
            "Epoch 00189: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.0265e-04 - val_loss: 7.8991e-05\n",
            "Epoch 190/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1009e-04\n",
            "Epoch 00190: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 38s 624ms/step - loss: 1.1009e-04 - val_loss: 9.9358e-05\n",
            "Epoch 191/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1753e-04\n",
            "Epoch 00191: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 42s 693ms/step - loss: 1.1753e-04 - val_loss: 9.4294e-05\n",
            "Epoch 192/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0471e-04\n",
            "Epoch 00192: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 569ms/step - loss: 1.0471e-04 - val_loss: 8.1905e-05\n",
            "Epoch 193/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0699e-04\n",
            "Epoch 00193: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.0699e-04 - val_loss: 1.0312e-04\n",
            "Epoch 194/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1167e-04\n",
            "Epoch 00194: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 569ms/step - loss: 1.1167e-04 - val_loss: 7.5383e-05\n",
            "Epoch 195/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0628e-04\n",
            "Epoch 00195: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 571ms/step - loss: 1.0628e-04 - val_loss: 8.1050e-05\n",
            "Epoch 196/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1245e-04\n",
            "Epoch 00196: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.1245e-04 - val_loss: 8.2324e-05\n",
            "Epoch 197/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0234e-04\n",
            "Epoch 00197: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 571ms/step - loss: 1.0234e-04 - val_loss: 9.6193e-05\n",
            "Epoch 198/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0029e-04\n",
            "Epoch 00198: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.0029e-04 - val_loss: 1.1547e-04\n",
            "Epoch 199/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0202e-04\n",
            "Epoch 00199: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.0202e-04 - val_loss: 8.2519e-05\n",
            "Epoch 200/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0278e-04\n",
            "Epoch 00200: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.0278e-04 - val_loss: 1.0399e-04\n",
            "Epoch 201/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0333e-04\n",
            "Epoch 00201: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.0333e-04 - val_loss: 7.0095e-05\n",
            "Epoch 202/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2093e-04\n",
            "Epoch 00202: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 569ms/step - loss: 1.2093e-04 - val_loss: 1.5578e-04\n",
            "Epoch 203/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1171e-04\n",
            "Epoch 00203: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.1171e-04 - val_loss: 7.1944e-05\n",
            "Epoch 204/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1575e-04\n",
            "Epoch 00204: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.1575e-04 - val_loss: 1.0135e-04\n",
            "Epoch 205/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2024e-04\n",
            "Epoch 00205: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.2024e-04 - val_loss: 8.7802e-05\n",
            "Epoch 206/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0404e-04\n",
            "Epoch 00206: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.0404e-04 - val_loss: 8.4229e-05\n",
            "Epoch 207/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3055e-04\n",
            "Epoch 00207: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.3055e-04 - val_loss: 7.0483e-05\n",
            "Epoch 208/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0978e-04\n",
            "Epoch 00208: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.0978e-04 - val_loss: 8.4216e-05\n",
            "Epoch 209/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0386e-04\n",
            "Epoch 00209: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 571ms/step - loss: 1.0386e-04 - val_loss: 1.1248e-04\n",
            "Epoch 210/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0605e-04\n",
            "Epoch 00210: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 573ms/step - loss: 1.0605e-04 - val_loss: 1.2374e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 211/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1320e-04\n",
            "Epoch 00211: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.1320e-04 - val_loss: 9.0782e-05\n",
            "Epoch 212/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.8639e-05\n",
            "Epoch 00212: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 9.8639e-05 - val_loss: 1.1636e-04\n",
            "Epoch 213/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0874e-04\n",
            "Epoch 00213: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 569ms/step - loss: 1.0874e-04 - val_loss: 7.7250e-05\n",
            "Epoch 214/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0464e-04\n",
            "Epoch 00214: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.0464e-04 - val_loss: 8.5782e-05\n",
            "Epoch 215/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.8913e-05\n",
            "Epoch 00215: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 9.8913e-05 - val_loss: 8.8239e-05\n",
            "Epoch 216/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1256e-04\n",
            "Epoch 00216: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.1256e-04 - val_loss: 1.3650e-04\n",
            "Epoch 217/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1778e-04\n",
            "Epoch 00217: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 565ms/step - loss: 1.1778e-04 - val_loss: 1.0830e-04\n",
            "Epoch 218/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1861e-04\n",
            "Epoch 00218: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 1.1861e-04 - val_loss: 8.2716e-05\n",
            "Epoch 219/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0461e-04\n",
            "Epoch 00219: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.0461e-04 - val_loss: 1.0088e-04\n",
            "Epoch 220/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0339e-04\n",
            "Epoch 00220: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.0339e-04 - val_loss: 8.0353e-05\n",
            "Epoch 221/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0668e-04\n",
            "Epoch 00221: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.0668e-04 - val_loss: 1.3981e-04\n",
            "Epoch 222/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0362e-04\n",
            "Epoch 00222: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.0362e-04 - val_loss: 1.1886e-04\n",
            "Epoch 223/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0150e-04\n",
            "Epoch 00223: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 565ms/step - loss: 1.0150e-04 - val_loss: 1.2890e-04\n",
            "Epoch 224/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0578e-04\n",
            "Epoch 00224: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 565ms/step - loss: 1.0578e-04 - val_loss: 7.5929e-05\n",
            "Epoch 225/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1433e-04\n",
            "Epoch 00225: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 566ms/step - loss: 1.1433e-04 - val_loss: 8.9435e-05\n",
            "Epoch 226/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0492e-04\n",
            "Epoch 00226: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.0492e-04 - val_loss: 1.4395e-04\n",
            "Epoch 227/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2045e-04\n",
            "Epoch 00227: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 566ms/step - loss: 1.2045e-04 - val_loss: 8.4540e-05\n",
            "Epoch 228/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0341e-04\n",
            "Epoch 00228: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 566ms/step - loss: 1.0341e-04 - val_loss: 9.8151e-05\n",
            "Epoch 229/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0095e-04\n",
            "Epoch 00229: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 569ms/step - loss: 1.0095e-04 - val_loss: 1.2178e-04\n",
            "Epoch 230/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1942e-04\n",
            "Epoch 00230: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.1942e-04 - val_loss: 1.2619e-04\n",
            "Epoch 231/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1956e-04\n",
            "Epoch 00231: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.1956e-04 - val_loss: 8.0550e-05\n",
            "Epoch 232/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1469e-04\n",
            "Epoch 00232: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.1469e-04 - val_loss: 9.2146e-05\n",
            "Epoch 233/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.8259e-05\n",
            "Epoch 00233: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 9.8259e-05 - val_loss: 1.0221e-04\n",
            "Epoch 234/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0579e-04\n",
            "Epoch 00234: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 1.0579e-04 - val_loss: 1.1357e-04\n",
            "Epoch 235/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0775e-04\n",
            "Epoch 00235: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 566ms/step - loss: 1.0775e-04 - val_loss: 1.1970e-04\n",
            "Epoch 236/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.6800e-05\n",
            "Epoch 00236: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 9.6800e-05 - val_loss: 1.1218e-04\n",
            "Epoch 237/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0109e-04\n",
            "Epoch 00237: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 566ms/step - loss: 1.0109e-04 - val_loss: 9.5115e-05\n",
            "Epoch 238/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0094e-04\n",
            "Epoch 00238: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 566ms/step - loss: 1.0094e-04 - val_loss: 8.6530e-05\n",
            "Epoch 239/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0371e-04\n",
            "Epoch 00239: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.0371e-04 - val_loss: 9.1952e-05\n",
            "Epoch 240/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1417e-04\n",
            "Epoch 00240: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 566ms/step - loss: 1.1417e-04 - val_loss: 1.0136e-04\n",
            "Epoch 241/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1154e-04\n",
            "Epoch 00241: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 566ms/step - loss: 1.1154e-04 - val_loss: 7.4465e-05\n",
            "Epoch 242/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1140e-04\n",
            "Epoch 00242: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.1140e-04 - val_loss: 8.4470e-05\n",
            "Epoch 243/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0462e-04\n",
            "Epoch 00243: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 571ms/step - loss: 1.0462e-04 - val_loss: 8.1093e-05\n",
            "Epoch 244/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0496e-04\n",
            "Epoch 00244: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 570ms/step - loss: 1.0496e-04 - val_loss: 1.4037e-04\n",
            "Epoch 245/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0593e-04\n",
            "Epoch 00245: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 563ms/step - loss: 1.0593e-04 - val_loss: 8.2555e-05\n",
            "Epoch 246/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0574e-04\n",
            "Epoch 00246: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 1.0574e-04 - val_loss: 8.4191e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 247/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0390e-04\n",
            "Epoch 00247: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 567ms/step - loss: 1.0390e-04 - val_loss: 1.1186e-04\n",
            "Epoch 248/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0690e-04\n",
            "Epoch 00248: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 565ms/step - loss: 1.0690e-04 - val_loss: 1.0004e-04\n",
            "Epoch 249/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 9.8358e-05\n",
            "Epoch 00249: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 34s 565ms/step - loss: 9.8358e-05 - val_loss: 9.6534e-05\n",
            "Epoch 250/250\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1005e-04\n",
            "Epoch 00250: val_loss did not improve from 0.00007\n",
            "61/61 [==============================] - 35s 568ms/step - loss: 1.1005e-04 - val_loss: 1.0342e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd4bc9e1c0>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train_close,y_train_close,validation_data=(X_test_close,y_test_close),epochs=250,batch_size=64,callbacks=callbacks,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW48P92OHTzk"
      },
      "outputs": [],
      "source": [
        "# import keras\n",
        "# model = keras.models.load_model(\"/home/personal/Desktop/best_5min.hdf5\")\n",
        "train_predict=model.predict(X_train_close)\n",
        "test_predict=model.predict(X_test_close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SueAYWe4cefo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz7tH5dkHT2p"
      },
      "outputs": [],
      "source": [
        "train_predict=scaler_close.inverse_transform(train_predict)\n",
        "test_predict=scaler_close.inverse_transform(test_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5BOFU9nHT52",
        "outputId": "67f4caeb-c1e8-45c8-a7ce-728f63b7a8de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "141.80497233226404"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_train_close1 = y_train_close.reshape(-1,1)\n",
        "y_train_close1 = scaler_close.inverse_transform(y_train_close1)\n",
        "math.sqrt(mean_squared_error(y_train_close1,train_predict))\n",
        "#187.74900342699905\n",
        "#15.267288828570317"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZKXfpc4bVkv",
        "outputId": "ae736933-76fa-45e9-e062-f90a45eed1bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "139.84743754633624"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test_close1 = y_test_close.reshape(-1,1)\n",
        "y_test_close1 = scaler_close.inverse_transform(y_test_close1)\n",
        "math.sqrt(mean_squared_error(y_test_close1,test_predict))\n",
        "#182.76988389047486\n",
        "#18.19369529558874"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "EGjnQaRNHT8r",
        "outputId": "fb36adb4-a2c6-4fdb-981c-d79dc1b91e2b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "look_back=100\n",
        "trainPredictPlot = numpy.empty_like(df_close)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(df_close)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(train_predict)+(look_back*2)+1:len(df_close)-1, :] = test_predict\n",
        "# plot baseline and predictions\n",
        "plt.plot(scaler_close.inverse_transform(df_close))\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhhOfH51HT_k",
        "outputId": "a747f39e-b1df-494f-a913-b47fa03c9e7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scaler.gz']"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.gz')\n",
        "#my_scaler = joblib.load('scaler.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uab2SF4-HUCV"
      },
      "outputs": [],
      "source": [
        "from tvDatafeed import TvDatafeed,Interval\n",
        "import time\n",
        "tv=TvDatafeed(auto_login=False, chromedriver_path='/usr/bin/chromedriver')\n",
        "final_predictions = []\n",
        "textfile = open(\"predictions_file.txt\", \"w\")   \n",
        "    \n",
        "\n",
        "for i in range(15):\n",
        "    btc_data = tv.get_hist('BTCUSDT','BINANCE',interval=Interval.in_5_minute,n_bars=101)\n",
        "    btc_data.to_csv('btc_5min_new1.csv',index=False)\n",
        "\n",
        "    pdf = pd.read_csv('btc_5min_new1.csv')\n",
        "    pdf = pdf.head(100)\n",
        "    pdf_high=pdf.reset_index()['high']\n",
        "    pdf_open=pdf.reset_index()['open']\n",
        "    pdf_close=pdf.reset_index()['close']\n",
        "    pdf_volume=pdf.reset_index()['volume']\n",
        "    pdf_low=pdf.reset_index()['low']\n",
        "    pdf_high1, pdf_open1,pdf_close1,pdf_volume1,pdf_low1 = [], [],[],[],[]\n",
        "\n",
        "    a = pdf_high[:]  \n",
        "    pdf_high1.append(a)\n",
        "    pdf_high1=numpy.array(pdf_high1)\t\n",
        "\n",
        "    a = pdf_open[:]  \n",
        "    pdf_open1.append(a)\n",
        "    pdf_open1=numpy.array(pdf_open1)\t\n",
        "\n",
        "    a = pdf_close[:]  \n",
        "    pdf_close1.append(a)\n",
        "    pdf_close1=numpy.array(pdf_close1)\t\n",
        "\n",
        "    a = pdf_volume[:]  \n",
        "    pdf_volume1.append(a)\n",
        "    pdf_volume1=numpy.array(pdf_volume1)\t\n",
        "\n",
        "    a = pdf_low[:]  \n",
        "    pdf_low1.append(a)\n",
        "    pdf_low1=numpy.array(pdf_low1)\t\n",
        "\n",
        "    pdf_high1 = scaler_high.fit_transform(np.array(pdf_high1).reshape(-1,1))\n",
        "    pdf_open1 = scaler_open.fit_transform(np.array(pdf_open).reshape(-1,1))\n",
        "    pdf_volume1 = scaler_volume.fit_transform(np.array(pdf_volume).reshape(-1,1))\n",
        "    pdf_low1 = scaler_low.fit_transform(np.array(pdf_low).reshape(-1,1))\n",
        "    pdf_close1 = scaler_close.fit_transform(np.array(pdf_close).reshape(-1,1))\n",
        "\n",
        "    pdf_high1 = pdf_high1.reshape(1,100 , 1)\n",
        "    pdf_open1 = pdf_open1.reshape(1,100, 1)\n",
        "    pdf_close1 = pdf_close1.reshape(1,100, 1)\n",
        "    pdf_volume1 = pdf_volume1.reshape(1,100, 1)\n",
        "    pdf_low1 = pdf_low1.reshape(1,100, 1)\n",
        "\n",
        "    ans = model.predict(pdf_close1)\n",
        "    ans= ans.reshape(-1,1)\n",
        "    ans = scaler_close.inverse_transform(ans)\n",
        "    final_predictions.append(ans)\n",
        "    t = time.localtime()\n",
        "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    textfile.write(str(current_time) +\"  \"+str(ans)+ \"\\n\")\n",
        "    time.sleep(300)\n",
        "    \n",
        "textfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5Cdqzml5u4_"
      },
      "outputs": [],
      "source": [
        "# time:14.16\n",
        "\n",
        "textfile = open(\"predictions_file.txt\", \"w\")\n",
        "for element in final_predictions:\n",
        "    textfile.write(str(element) + \"\\n\")\n",
        "textfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MeOPn3555Wl",
        "outputId": "b9d809e9-20ca-42b9-ba71-16da072a55d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[47982.527]], dtype=float32),\n",
              " array([[47828.613]], dtype=float32),\n",
              " array([[47786.125]], dtype=float32),\n",
              " array([[47796.543]], dtype=float32),\n",
              " array([[47536.19]], dtype=float32),\n",
              " array([[47571.977]], dtype=float32),\n",
              " array([[47741.45]], dtype=float32),\n",
              " array([[47776.26]], dtype=float32),\n",
              " array([[47517.98]], dtype=float32),\n",
              " array([[47631.508]], dtype=float32),\n",
              " array([[47795.637]], dtype=float32),\n",
              " array([[47770.625]], dtype=float32),\n",
              " array([[47581.98]], dtype=float32),\n",
              " array([[47656.176]], dtype=float32),\n",
              " array([[47643.96]], dtype=float32)]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAEsCff756VZ"
      },
      "outputs": [],
      "source": [
        "ans_final = []\n",
        "for i in range(len(final_predictions)):\n",
        "    ans_final.append(final_predictions[i][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-MF7lXt56YQ",
        "scrolled": true,
        "outputId": "63d87a15-1aab-4739-8edb-9df1cb2591cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efd7b562160>]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(ans_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gUpOGkE56ed"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}